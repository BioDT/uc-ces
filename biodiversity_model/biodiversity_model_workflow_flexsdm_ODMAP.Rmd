---
title: "BioDT biodiversity model"
date: "`r Sys.Date()`"
output: html_document
params:
  taxonkey: 5334220
  out_file: "untitled.tif"
  n_bootraps: 5
---


```{r, include=FALSE}
# testing 
params <- list(taxonkey= 5334220, out_file= "untitled.tif", n_bootraps= 6)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages

```{r packages, warning=FALSE}
#spatial
library(terra)
library(sf)

#statistical
library(flexsdm)

#getting gbif data
library(rgbif)

#spatial thinning
library(spThin)

#data manipulation
library(dplyr)

# Load markdown for ODMAP render
library(rmarkdown)

# For reading ODMAP template
library(readr)

set.seed(42)

```

## Load data

### Environmental

```{r load_environmental_data}
#Currently loading a raster file locally but this could be upgraded to any other raster
env_data <- rast("inputs/env-layers.tif")

env_data_low_res <- aggregate(env_data, fact = 10)

```

### Species data from GBIF

```{r}
# obtain GBIF username, password, and email credentials from R environ file
gbif_user <- Sys.getenv("GBIF_USER")
gbif_pwd <- Sys.getenv("GBIF_PWD")
gbif_email <- Sys.getenv("GBIF_EMAIL")

# Get the extent of the raster
raster_extent <- ext(env_data)

# Convert the extent to a WKT polygon
wkt_extent <- sprintf("POLYGON((%s %s, %s %s, %s %s, %s %s, %s %s))",
                      xmin(raster_extent), ymin(raster_extent), # Bottom left
                      xmax(raster_extent), ymin(raster_extent), # Bottom right
                      xmax(raster_extent), ymax(raster_extent), # Top right
                      xmin(raster_extent), ymax(raster_extent), # Top left
                      xmin(raster_extent), ymin(raster_extent)) # Close polygon

# Make a download request for Danaus plexippus occurrences in 2020
res <- occ_download(
  pred("taxonKey", params$taxonkey),
  pred_within(wkt_extent),
  pred("coordinateUncertaintyInMeters", "0,101"),
  pred("hasCoordinate", TRUE),
  pred("hasGeospatialIssue", FALSE)
)

# Initial check of the download status
status <- occ_download_meta(res)$status

# Keep checking the status until it's either "SUCCEEDED" or "FAILED"
while(status != "SUCCEEDED") {
  print("GBIF data preparation incomplete. Waiting 10 seconds longer...")
  Sys.sleep(10) # Wait for 10 seconds before checking again
  status <- occ_download_meta(res)$status # Update the status
}

# Delete all previous GBIF download data 
unlink("GBIF_data", recursive = TRUE)
dir.create("GBIF_data")

# Once the download is ready, you can download the file to your local system
occ_download_get(res, path = "GBIF_data", full.names = TRUE, overwrite = TRUE)

# Unzip and read the data
unzip(list.files("GBIF_data", full.names = TRUE)[1], exdir = "GBIF_data")
gbif_data = read.delim("GBIF_data/occurrence.txt", header = TRUE)

# Save doi
doi =  occ_download_meta(res)$doi
access_datetime = as.POSIXct(occ_download_meta(res)$created, format="%Y-%m-%dT%H:%M:%OS", tz = "UTC")

```

```{r load_gbif_data}

#get the appropriate columns
gbif_data <- gbif_data %>% select(scientificName, 
                                       phylum,
                                       order,
                                       family,
                                       y = decimalLatitude,
                                       x = decimalLongitude,
                                       occurrenceStatus,
                                       coordinateUncertaintyInMeters,
                                       year,
                                       month,
                                       day,
                                       license,
                                       recordedBy,
                                       identifiedBy,
                                       rightsHolder
                                       )
#set presence absence
gbif_data$pr_ab <- 0
gbif_data$pr_ab[gbif_data$occurrenceStatus == "PRESENT"] <- 1

gbif_xy <- gbif_data %>% select(x, y)

#Species name
target_species = gbif_data$scientificName %>% first()
print(target_species)
```

### FlexSDM workflow

#### Delimit of a calibration area

Delimiting the calibration area (aka accessible area) is an essential step in SDMs both in methodological and theoretical terms. The calibration area will affect several characteristics of a SDM like the range of environmental variables, the number of absences, the distribution of background points and pseudo-absences, and unfortunately, some performance metrics like AUC and TSS. There are several ways to delimit a calibration area. In calib_area(). We used a method that the calibration area is delimited by a 5-km buffer around presences (shown in the figure below).

```{r calibation_area}

ca <- calib_area(
    data = gbif_data,
    x = 'x',
    y = 'y',
    method =  c('buffer', width = 5000),
    crs = crs(env_data)
  ) # create a calibration area with 100 km buffer around occurrence points

# visualize the species occurrences
layer1 <- env_data_low_res[[8]]

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")

```

#### Occurrence filtering

Sample bias in species occurrence data has long been a recognized issue in SDM. However, environmental filtering of observation data can improve model predictions by reducing redundancy in environmental (e.g. climatic) hyper-space (Varela et al. 2014). Here we will use the function occfilt_env() to thin the red fir occurrences based on environmental space. This function is unique to flexsdm, and in contrast with other packages is able to use any number of environmental dimensions and does not perform a PCA before filtering.

Next we apply environmental occurrence filtering using 8 bins and display the resulting filtered occurrence data
```{r occ_filtering}

# I'm not sure we need this anymore, given that I have input the bounds of the cairngorms tif file into GBIF param search.
gbif_data$id <- 1:nrow(gbif_data) # adding unique id to each row
gbif_data_f <- gbif_data %>%
  occfilt_env(
    data = .,
    x = "x",
    y = "y",
    id = "id",
    nbins = 8,
    env_layer = env_data
  ) %>%
  left_join(gbif_data, by = c("id", "x", "y"))

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
```

#### Random partition with 4 folds

Data partitioning, or splitting data into testing and training groups, is a key step in building SDMs. flexsdm offers multiple options for data partitioning and here we use a spatial block method. Geographically structured data partitioning methods are especially useful if users want to evaluate model transferability to different regions or time periods. The part_sblock() function explores spatial blocks with different raster cells sizes and returns the one that is best suited for the input datset based on spatial autocorrelation, environmental similarity, and the number of presence/absence records in each block partition. The function’s output provides users with 1) a tibble with presence/absence locations and the assigned partition number, 2) a tibble with information about the best partition, and 3) a SpatRaster showing the selected grid. Here we want to divide the data into 4 different partitions using the spatial block method.

```{r partitioning}
set.seed(10)
gbif_data_f_partitioned <- gbif_data_f %>%
  part_random(
    data = .,
    pr_ab = "pr_ab",
    method = c(method = "kfold", folds = 4)
  )

# n points per block type
gbif_data_f_partitioned %>%
  dplyr::group_by(.part) %>%
  dplyr::count()
```

#### Sample background

The function sample_background() allows selection of background sample points based on different geographic restrictions and sampling methods. Here, we sample a set of background points based on our earlier spatial block partitioning using the “random” method. Using lapply() in this case ensures that we generate background points in each of our spatial blocks (n = 2). We are also specifying that we want ten times the amount of background points as our original occurrences and that our calibration area will be the buffer area around presence points (see section on “Calibration area”).

```{r background_samples}
set.seed(10)
bg <- gbif_data_f_partitioned %>% 
  split(f=gbif_data_f_partitioned$.part) %>%
  lapply(function(data){
    sample_background(
      data = data,
      x = "x",
      y = "y",
      n = nrow(data),
      # number of background points to be sampled
      method = "random",
      rlayer = env_data,
      calibarea = ca # A SpatVector which delimit the calibration area used for a given species
    ) %>%
      mutate(.part = data$.part[1])
      
  }) %>% 
  bind_rows()
  
gbif_data_pa <- bind_rows(gbif_data_f_partitioned, bg)

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
points(bg[,c("x", "y")], col = "red")

#extract data from raster
gbif_data_pa <- gbif_data_pa %>% sdm_extract(
    data = .,
    x = "x",
    y = "y",
    env_layer = env_data,
    filter_na = TRUE
  )

```

#### Model fitting

Now, fit our models. The flexsdm package offers a wide range of modeling options, from traditional statistical methods like GLMs and GAMs, to machine learning methods like random forests and support vector machines. For each modeling method, flexsdm provides both fit_ and tune_ functions, which allow users to use default settings or adjust hyperparameters depending on their research goals. Here, we will test out tune_max() (tuned Maximum Entropy model), fit_gau() (fit Guassian Process model), and fit_glm (fit Generalized Linear Model). For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson). In this example, we selected TSS as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model.

```{r model fitting}
f_gau <- fit_gau(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen")
)
f_gau$performance %>% kable()

f_glm <- fit_glm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  poly = 2
)
f_glm$performance %>% kable()

f_svm <- fit_svm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
)
f_svm$performance %>% kable()

```


#### Model veriabitiy

Calculate variability 

```{r model_variability}
#variability between model types
# individ_models <- sdm_predict(
#   models = list(f_gau,f_glm,f_svm),
#   pred = env_data,
#   thr = NULL,
#   con_thr = FALSE,
#   predict_area = NULL
# )
# 
# variability <- rast(individ_models) %>% diff() %>% abs() 
# plot(variability,col=cl)


```

#### Build ensemble model

Spatial predictions from different SDM algorithms can vary substantially, and ensemble modeling has become increasingly popular. With the fit_ensemble() function, users can easily produce an ensemble SDM based on any of the individual fit_ and tune_ models included the package. In this example, we fit an ensemble model for red fir based on the weighted average of the three individual models. We used the same threshold values and performance metric that were implemented in the individual models.

```{r build_ensemble}
ens_m <- fit_ensemble(
  models = list(f_gau, f_glm, f_svm),
  ens_method = "meanw",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  thr_model = "max_sens_spec",
  metric = "TSS"
)

model_perf <- sdm_summarize(list(f_gau, f_glm, f_svm, ens_m))
knitr::kable(model_perf)

```

#### Project the ensemble model

Next we project the ensemble model in space across the entire extent of our environmental layer, using the sdm_predict() function. This function can be use to predict species suitability across any area for species’ current or future suitability. In this example, we only project the ensemble model with one threshold, though users have the option to project multiple models with multiple threshold values. Here, we also specify that we want the function to return a SpatRast with continuous suitability values above the threshold (con_thr = TRUE).

```{r project_model}
pr_1 <- sdm_predict(
  models = ens_m,
  pred = env_data,
  thr = "max_sens_spec",
  con_thr = TRUE,
  predict_area = NULL
)
#> Predicting ensembles

unconstrained <- pr_1$meanw[[1]]
names(unconstrained) <- "unconstrained"

cl <- rev(c("#FDE725", "#B3DC2B", "#6DCC57", "#36B677", "#1F9D87", "#25818E", "#30678D", "#3D4988", "#462777", "#440154"))
plot(unconstrained, col=cl)
```

#### Constrain the model with msdm_posterior

Finally, flexsdm offers users function that help correct overprediction of SDM based on occurrence records and suitability patterns. In this example we constrained the ensemble model using the method “occurrence based restriction”, which assumes that suitable patches that intercept species occurrences are more likely a part of species distributions than suitable patches that do not intercept any occurrences. Because all methods of the msdm_posteriori() function work with presences it is important to always use the original database (i.e., presences that have not been spatially or environmentally filtered). All of the methods available in the msdm_posteriori() function are based on Mendes et al. (2020).

```{r constrain_model}
thr_val <- ens_m$performance %>%
  dplyr::filter(threshold == "max_sens_spec") %>%
  pull(thr_value)

m_pres <- msdm_posteriori(
  records = gbif_data,
  x = "x",
  y = "y",
  pr_ab = "pr_ab",
  cont_suit = pr_1$meanw[[1]],
  method = c("obr"),
  thr = c("sensitivity", sens = thr_val),
  buffer = NULL
)

constrained <- m_pres$meanw[[1]]
names(constrained) <- "constrained"
plot(constrained, col=cl)

```


#### Distance from nearest point

```{r distance_from_record}
# create a blank vector with band name 'distance' and values are NA
distance_rast <- env_data[[1]]
values(distance_rast) <-NA
names(distance_rast) <- "distance"

#set grid cells with values to non-NA value (-1)
values(distance_rast)[cellFromXY(distance_rast,xy=as.matrix(gbif_data[,c("x","y")]))] <- -1

#create a raster of distance from nearest record
distance_rast <- distance(distance_rast)/1000 #distance from cells in km
distance_rast_scaled <- min(distance_rast,5)/5 #cap a 5 km and scale to 0-1
plot(distance_rast_scaled)


# create a raster of suitable, but far from record
suitable_unrecorded <- unconstrained*distance_rast_scaled
plot(suitable_unrecorded, col=cl)
```

## Export

Export the model the output file location provided to the parametrised markdown

```{r export}
model_outputs <- constrained
model_outputs$suitable_unrecorded <-suitable_unrecorded
plot(model_outputs, col=cl)

#save raster
model_outputs %>% writeRaster(params$out_file,overwrite=T)

#save model
list(f_gau, f_glm, f_svm, ens_m) %>% saveRDS(gsub(".tif",".RDS",params$out_file))
```


## Generate ODMAP inputs

Here, we use ODMAP (Overview, Data, Model, Assessment and Prediction) to document our SDM model output and workflow
```{r}
# Load the functions I have created seperately for ODMAP generation
source("render_ODMAP.r")

render_ODMAP(
  o_authorship_1 = "BioDT species distribution models",
  o_authorship_3 = "simrol@ceh.ac.uk; dylcar@ceh.ac.uk",
  o_authorship_4 = "",
  o_objective_1 = "Mapping and interpolation",
  o_objective_2 = "Maps of species presence",
  o_location_1 = "Cairngorms National Park, United Kingdom",
  o_scale_2 = "0.1 m",
  o_scale_3 = "There is no temporal extent to the analysis",
  o_scale_4 = "There is no temporal resolution to the analysis",
  o_scale_5 = "political",
  o_bio_1 = "citizen science; field survey",
  o_bio_2 = "point occurrence",
  o_concept_1 = paste("investigating how environment variables affect the distributions of the species,", target_species, "in the Cairngorms National Park"),
  o_assumptions_1 = "We assume that there is no temporal changes in environmental variables, that the abiotic variables is the sole predictor of species distributions other than biotic variables",
  o_algorithms_1 = "glm; svm; gaussian process model",
  o_algorithms_2 = "A variety of models were used without making run time too extensive",
  o_algorithms_3 = "We calculated a mean weighted average based on model performance",
  o_workflow_1 = "Species occurrence data from the Cairngorms, Scotland was obtained by download from GBIF. We filtered environmental variables to only include environment data from within a 5 km buffer of recorded occurrences, and conducted spatial thinning. Using 4-fold partitioning, a series of spatial models were developed and validated. An ensemble model was created from the model series. We corrected for overprediction using posteriori methods",
  o_software_2 = "https://github.com/BioDT/uc-ces/tree/main/biodiversity_model",
  o_software_3 = paste("data obtained from GBIF API with DOI:", doi),
  d_bio_2 = "GBIF taxonomic backbone",
  d_bio_3 = "species",
  d_bio_5 = "opportunistic data",
  d_bio_7 = "No mask was used",
  d_bio_9 = "no cleaning/filtering steps",
  d_bio_10 = "not applicable",
  d_bio_12 = "",
  d_part_2 = "we calculate TSS, the threshold at which sensitivity and specificity are equal, as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model",
  d_pred_2 = "Google earth engine",
  d_pred_4 = "0.1 km",
  d_pred_5 = crs(env_data, describe = T)$name,
  d_pred_6 = "No temporal extent",
  d_pred_7 = "No temporal resolution",
  d_pred_8 = "no upscaling/downscaling",
  d_pred_9 = "",
  d_pred_10 = "",
  d_proj_2_xmin = "Not applicable",
  d_proj_2_xmax = "Not applicable",
  d_proj_2_ymin = "Not applicable",
  d_proj_2_ymax = "Not applicable",
  d_proj_3 = "Not applicable",
  d_proj_4 = "Not applicable",
  d_proj_5 = "Not applicable",
  d_proj_6 = "Not applicable",
  d_proj_7 = "Not applicable",
  d_proj_8 = "Not applicable",
  m_preselect_1 = "Not applicable",
  m_multicol_1 = "No methods used to handle collinearity",
  m_settings_2 = "Not applicable",
  m_estim_1 = "Not applicable",
  m_estim_2 = "No quantification",
  m_estim_3 = "No assessment",
  m_selection_1 = "We included all environment variables recorded in a model input raster spanning the Cairngorms, Scotland",
  m_selection_2 = "No variable weights were used",
  m_selection_3 = "Occurrences obtained from the Global Biodiversity Information Facility (GBIF), with pseudo replication of absences. See model settings table for model classes and parameters",
  m_depend_1 = "No method",
  m_depend_2 = "No method",
  m_depend_3 = "No method",
  m_threshold_1 = "Not applicable",
  a_perform_1 = "Not applicable",
  a_perform_3 = "Not applicable.",
  a_plausibility_1 = "No response plots",
  a_plausibility_2 = "No expert judgements",
  p_output_1 = "Species proportional occurrence",
  p_uncertainty_1 = "Not applicable",
  p_uncertainty_2 = "The models are trained using GBIF datasets. There may be biases introduced by the method(s) of data collection and source contributor(s)",
  p_uncertainty_3 = "Not applicable",
  p_uncertainty_4 = "",
  p_uncertainty_5 = "No visualization or treatment"
)

# Open the script
shell.exec("ODMAP_report.docx")
```