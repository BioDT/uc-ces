---
title: "BioDT biodiversity model"
date: "`r Sys.Date()`"
output: html_document
params:
  taxonkey: 5640571
  out_file: "untitled.tif"
  n_bootraps: 5
---


```{r, include=FALSE}
# testing 
params <- list(taxonkey= 8211070, out_file= "untitled.tif", n_bootraps= 6)

# Red squirrel: 8211070
# badger 2433875

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages

```{r packages, warning=FALSE}
#spatial
library(terra)
library(sf)

#statistical
library(flexsdm)

#getting gbif data
library(rgbif)

#spatial thinning
library(spThin)

#data manipulation
library(dplyr)

# Load markdown for ODMAP render
library(rmarkdown)

# For reading ODMAP template
library(readr)

set.seed(42)

```

## Load data

### Environmental

```{r load_environmental_data}
#Currently loading a raster file locally but this could be upgraded to any other raster
env_data <- rast("inputs/upscaled_scotland/bio_image.tif")
# names(env_data)[15] = "pop_density"
# env_data = subset(env_data, 1:5)
#env_data_low_res <- aggregate(env_data, fact = 10)
```

### Species data from GBIF

```{r}
# obtain GBIF username, password, and email credentials from R environ file
gbif_user <- Sys.getenv("GBIF_USER")
gbif_pwd <- Sys.getenv("GBIF_PWD")
gbif_email <- Sys.getenv("GBIF_EMAIL")

# Get the extent of the raster
raster_extent <- ext(env_data)

# # Convert the extent to a WKT polygon
# wkt_extent <- sprintf("POLYGON((%s %s, %s %s, %s %s, %s %s, %s %s))",
#                       xmin(raster_extent), ymin(raster_extent), # Bottom left
#                       xmax(raster_extent), ymin(raster_extent), # Bottom right
#                       xmax(raster_extent), ymax(raster_extent), # Top right
#                       xmin(raster_extent), ymax(raster_extent), # Top left
#                       xmin(raster_extent), ymin(raster_extent)) # Close polygon

# # Make a download request for Danaus plexippus occurrences in 2020
# res <- occ_download(
#   pred("taxonKey", params$taxonkey),
#   pred_within(wkt_extent),
#   pred("coordinateUncertaintyInMeters", "0,101"),
#   pred("hasCoordinate", TRUE),
#   pred("hasGeospatialIssue", FALSE)
# )

# # Initial check of the download status
# status <- occ_download_meta(res)$status

# # Keep checking the status until it's either "SUCCEEDED" or "FAILED"
# while(status != "SUCCEEDED") {
#   print("GBIF data preparation incomplete. Waiting 10 seconds longer...")
#   Sys.sleep(10) # Wait for 10 seconds before checking again
#   status <- occ_download_meta(res)$status # Update the status
# }

# # Delete all previous GBIF download data 
# unlink("GBIF_data", recursive = TRUE)
# dir.create("GBIF_data")

# # Once the download is ready, you can download the file to your local system
# occ_download_get(res, path = "GBIF_data", full.names = TRUE, overwrite = TRUE)

# # Unzip and read the data
# unzip(list.files("GBIF_data", full.names = TRUE)[1], exdir = "GBIF_data")
gbif_data = read.delim("GBIF_data/occurrence.txt", header = TRUE)

# Save doi
# doi =  occ_download_meta(res)$doi
# access_datetime = as.POSIXct(occ_download_meta(res)$created, format="%Y-%m-%dT%H:%M:%OS", tz = "UTC")

```

```{r load_gbif_data}

#get the appropriate columns
gbif_data <- gbif_data %>% select(scientificName, 
                                       phylum,
                                       order,
                                       family,
                                       y = decimalLatitude,
                                       x = decimalLongitude,
                                       occurrenceStatus,
                                       coordinateUncertaintyInMeters,
                                       year,
                                       month,
                                       day,
                                       license,
                                       recordedBy,
                                       identifiedBy,
                                       rightsHolder
                                       )
#set presence absence
gbif_data$pr_ab <- 0
gbif_data$pr_ab[gbif_data$occurrenceStatus == "PRESENT"] <- 1

gbif_xy <- gbif_data %>% select(x, y)

#Species name
target_species = gbif_data$scientificName %>% first()
print(target_species)
```

A challenge about extracting the extent of the scotland environmental raster is that rasters are confined to square or rectangular shapes. Therefore to obtain the whole of scotland, we receive data from northern ireland and northern England. To fix this, we must load a shapefile of countries in the UK, filter for scotland, and filter the GBIF data to only include observations within the boundaries of Scotland.

```{r filter GBIF data to scotland}
### Load Scotland boundary shapefile
# Assuming you have a shapefile for Scotland boundaries, load it
scotland_boundary <- st_read("inputs/upscaled_scotland/infuse_ctry_2011.shp") %>% filter(name == "Scotland")

### Transform GBIF data to an sf object for spatial operations
gbif_sf <- st_as_sf(gbif_data, coords = c("x", "y"), crs = 4326)

### Ensure both layers are in the same CRS
scotland_boundary <- st_transform(scotland_boundary, crs = st_crs(gbif_sf))

### Filter GBIF data points to include only those within the Scotland boundary
gbif_within_scotland <- st_intersection(gbif_sf, scotland_boundary)

### Convert the filtered sf object back to a dataframe
gbif_data <- as.data.frame(st_coordinates(gbif_within_scotland)) %>% rename(x = X, y = Y)
gbif_data <- cbind(gbif_data, st_drop_geometry(gbif_within_scotland))

# Print the number of points after cropping
print(paste("Number of points after cropping:", nrow(gbif_data)))
```

### FlexSDM workflow

#### Delimit of a calibration area

Delimiting the calibration area (aka accessible area) is an essential step in SDMs both in methodological and theoretical terms. The calibration area will affect several characteristics of a SDM like the range of environmental variables, the number of absences, the distribution of background points and pseudo-absences, and unfortunately, some performance metrics like AUC and TSS. There are several ways to delimit a calibration area. In calib_area(). We used a method that the calibration area is delimited by a 5-km buffer around presences (shown in the figure below).

```{r calibation_area}

ca <- calib_area(
    data = gbif_data,
    x = 'x',
    y = 'y',
    method =  c('buffer', width = 5000),
    crs = crs(env_data)
  ) # create a calibration area with 100 km buffer around occurrence points

layer1 <- env_data[[1]]

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")

```

#### Occurrence filtering

Sample bias in species occurrence data has long been a recognized issue in SDM. However, environmental filtering of observation data can improve model predictions by reducing redundancy in environmental (e.g. climatic) hyper-space (Varela et al. 2014). Here we will use the function occfilt_env() to thin the red fir occurrences based on environmental space. This function is unique to flexsdm, and in contrast with other packages is able to use any number of environmental dimensions and does not perform a PCA before filtering.

Next we apply environmental occurrence filtering using 8 bins and display the resulting filtered occurrence data
```{r occ_filtering}

# Filter occurences to reduce sampling biases
gbif_data$id <- 1:nrow(gbif_data) # adding unique id to each row
gbif_data_f <- gbif_data %>%
  occfilt_env(
    data = .,
    x = "x",
    y = "y",
    id = "id",
    nbins = 8,
    env_layer = env_data
  ) %>%
  left_join(gbif_data, by = c("id", "x", "y"))

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
```

#### Random partition with 4 folds

Data partitioning, or splitting data into testing and training groups, is a key step in building SDMs. flexsdm offers multiple options for data partitioning and here we use a spatial block method. Geographically structured data partitioning methods are especially useful if users want to evaluate model transferability to different regions or time periods. The part_sblock() function explores spatial blocks with different raster cells sizes and returns the one that is best suited for the input datset based on spatial autocorrelation, environmental similarity, and the number of presence/absence records in each block partition. The function’s output provides users with 1) a tibble with presence/absence locations and the assigned partition number, 2) a tibble with information about the best partition, and 3) a SpatRaster showing the selected grid. Here we want to divide the data into 4 different partitions using the spatial block method.

```{r partitioning}
set.seed(10)
gbif_data_f_partitioned <- gbif_data_f %>%
  part_random(
    data = ,
    pr_ab = "pr_ab",
    method = c(method = "kfold", folds = 4)
  )

# n points per block type
gbif_data_f_partitioned %>%
  dplyr::group_by(.part) %>%
  dplyr::count()
```

#### Sample background

The function sample_background() allows selection of background sample points based on different geographic restrictions and sampling methods. Here, we sample a set of background points based on our earlier spatial block partitioning using the “random” method. Using lapply() in this case ensures that we generate background points in each of our spatial blocks (n = 2). We are also specifying that we want ten times the amount of background points as our original occurrences and that our calibration area will be the buffer area around presence points (see section on “Calibration area”).

```{r background_samples}
set.seed(10)
bg <- gbif_data_f_partitioned %>% 
  split(f=gbif_data_f_partitioned$.part) %>%
  lapply(function(data){
    sample_background(
      data = data,
      x = "x",
      y = "y",
      n = nrow(data),
      # number of background points to be sampled
      method = "random",
      rlayer = env_data,
      calibarea = ca # A SpatVector which delimit the calibration area used for a given species
    ) %>%
      mutate(.part = data$.part[1])
      
  }) %>% 
  bind_rows()
  
gbif_data_pa <- bind_rows(gbif_data_f_partitioned, bg)

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
points(bg[,c("x", "y")], col = "red")

#extract data from raster
gbif_data_pa <- gbif_data_pa %>% sdm_extract(
    data = .,
    x = "x",
    y = "y",
    env_layer = env_data,
    filter_na = TRUE
  )

```

#### Model fitting

Now, fit our models. The flexsdm package offers a wide range of modeling options, from traditional statistical methods like GLMs and GAMs, to machine learning methods like random forests and support vector machines. For each modeling method, flexsdm provides both fit_ and tune_ functions, which allow users to use default settings or adjust hyperparameters depending on their research goals. Here, we will test out tune_max() (tuned Maximum Entropy model), fit_gau() (fit Guassian Process model), and fit_glm (fit Generalized Linear Model). For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson). In this example, we selected TSS as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model.

```{r model fitting}

f_gau <- fit_gau(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen")
)

f_gau$performance %>% kable()

f_glm <- fit_glm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  poly = 2
)
f_glm$performance %>% kable()

f_svm <- fit_svm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
)
f_svm$performance %>% kable()

```


#### Model veriabitiy

Calculate variability 

```{r model_variability}
#variability between model types
# individ_models <- sdm_predict(
#   models = list(f_gau,f_glm,f_svm),
#   pred = env_data,
#   thr = NULL,
#   con_thr = FALSE,
#   predict_area = NULL
# )
# 
# variability <- rast(individ_models) %>% diff() %>% abs() 
# plot(variability,col=cl)

```

#### Build ensemble model

Spatial predictions from different SDM algorithms can vary substantially, and ensemble modeling has become increasingly popular. With the fit_ensemble() function, users can easily produce an ensemble SDM based on any of the individual fit_ and tune_ models included the package. In this example, we fit an ensemble model for red fir based on the weighted average of the three individual models. We used the same threshold values and performance metric that were implemented in the individual models.

```{r build_ensemble}
ens_m <- fit_ensemble(
  models = list(f_gau, f_glm, f_svm),
  ens_method = "meanw",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  thr_model = "max_sens_spec",
  metric = "TSS"
)

model_perf <- sdm_summarize(list(f_gau, f_glm, f_svm, ens_m))
knitr::kable(model_perf)

```

#### Project the ensemble model

Next we project the ensemble model in space across the entire extent of our environmental layer, using the sdm_predict() function. This function can be use to predict species suitability across any area for species’ current or future suitability. In this example, we only project the ensemble model with one threshold, though users have the option to project multiple models with multiple threshold values. Here, we also specify that we want the function to return a SpatRast with continuous suitability values above the threshold (con_thr = TRUE).

```{r project_model}
pr_1 <- sdm_predict(
  models = ens_m,
  pred = env_data,
  thr = "max_sens_spec",
  con_thr = TRUE,
  predict_area = NULL
)

unconstrained <- pr_1$meanw[[1]]
names(unconstrained) <- "unconstrained"

cl <- rev(c("#FDE725", "#B3DC2B", "#6DCC57", "#36B677", "#1F9D87", "#25818E", "#30678D", "#3D4988", "#462777", "#440154"))
plot(unconstrained, col=cl)
```

#### Constrain the model with msdm_posterior

Finally, flexsdm offers users function that help correct overprediction of SDM based on occurrence records and suitability patterns. In this example we constrained the ensemble model using the method “occurrence based restriction”, which assumes that suitable patches that intercept species occurrences are more likely a part of species distributions than suitable patches that do not intercept any occurrences. Because all methods of the msdm_posteriori() function work with presences it is important to always use the original database (i.e., presences that have not been spatially or environmentally filtered). All of the methods available in the msdm_posteriori() function are based on Mendes et al. (2020).

```{r constrain_model}
thr_val <- ens_m$performance %>%
  dplyr::filter(threshold == "max_sens_spec") %>%
  pull(thr_value)

m_pres <- msdm_posteriori(
  records = gbif_data,
  x = "x",
  y = "y",
  pr_ab = "pr_ab",
  cont_suit = pr_1$meanw[[1]],
  method = c("obr"),
  thr = c("sensitivity", sens = thr_val),
  buffer = NULL
)

# Error: cannot allocate vector of size 30.0 Gb
constrained <- m_pres$meanw[[1]]
names(constrained) <- "constrained"
plot(constrained, col=cl)

```


#### Distance from nearest point

```{r distance_from_record}
# create a blank vector with band name 'distance' and values are NA
distance_rast <- env_data[[1]]
values(distance_rast) <-NA
names(distance_rast) <- "distance"

#set grid cells with values to non-NA value (-1)
values(distance_rast)[cellFromXY(distance_rast,xy=as.matrix(gbif_data[,c("x","y")]))] <- -1

#create a raster of distance from nearest record
distance_rast <- distance(distance_rast)/1000 #distance from cells in km
distance_rast_scaled <- min(distance_rast,5)/5 #cap a 5 km and scale to 0-1
plot(distance_rast_scaled)


# create a raster of suitable, but far from record
suitable_unrecorded <- unconstrained*distance_rast_scaled
plot(suitable_unrecorded, col=cl)
```

## Export

Export the model the output file location provided to the parametrised markdown

```{r export}
model_outputs <- constrained
model_outputs$suitable_unrecorded <-suitable_unrecorded
plot(model_outputs, col=cl)

#save raster
model_outputs %>% writeRaster(params$out_file,overwrite=T)

#save model
list(f_gau, f_glm, f_svm, ens_m) %>% saveRDS(gsub(".tif",".RDS",params$out_file))
```


## Generate ODMAP inputs

Here, we use ODMAP (Overview, Data, Model, Assessment and Prediction) to document our SDM model output and workflow

```{r, message = FALSE, warning = FALSE}
input_list = readRDS(file.path(getwd(), "ODMAP_template_list.RDS"))

# Generate template dictionary
odmap_dict <- read_csv("odmap_dict.csv")
completed_fields <- read.csv("C:/Users/dylcar/OneDrive - UKCEH/Desktop/BioDT/uc-ces/metadata/biodiversity_model_od_map_fields.csv")
odmap_dict <- left_join(odmap_dict, completed_fields)

# Load your SpatRaster object (replace 'raster_object' with your actual object)
output_raster_object <- rast("untitled.tif")

# Define a function to search the script code use
script_search <- function(regex) {
lines <- readLines("biodiversity_model_workflow_flexsdm_ODMAP.Rmd")
any(sapply(lines, FUN = function(line){
  trimmed_line <- trimws(line) # Remove leading and trailing whitespace
  searches = !startsWith(trimmed_line, "#") && grepl(regex, trimmed_line) && !grepl("script_search", trimmed_line)
}))
}

# Function to obtain a description for a given ID from a dictionary data frame
obtain_description <- function(id) {
  # Check if the ID exists in the dictionary
  if (!(id %in% odmap_dict$element_id)) {
    last_underscore <- max(gregexpr("_", id)[[1]])
    id <- substr(id, 1, last_underscore - 1)
  }

  # Retrieve description and text entry for the ID
  description = odmap_dict %>% filter(element_id == id) %>% pull(element_placeholder)
  text_entry = odmap_dict %>% filter(element_id == id) %>% pull(Value)

  # Print the description and text entry if available
  if (!is.na(text_entry) && identical(text_entry, character(0)) == FALSE && text_entry != "") {
    print(paste0(description, "    Entry found: ", text_entry))
  } else {
    print(description)
  }
}

# Function to obtain elements in each subsection from a dictionary data frame
obtain_subsection_ids <- function(section_name, subsection_text) {
  # Filter the dictionary for the given subsection
  odmap_dict_sub = odmap_dict %>% filter(section == section_name, subsection == subsection_text)
  elements = odmap_dict_sub$element_id

  # Process names in the input list
  input_list_stemmed_names = unlist(sapply(names(input_list), USE.NAMES = F, FUN = function(name) {
    last_underscore <- max(gregexpr("_", name)[[1]])
    substr(name, 1, last_underscore - 1)
  }))

  # Print the names of elements in the subsection
  for (i in seq_along(elements)) {

    nums = which(input_list_stemmed_names == elements[i])

    if (length(nums) == 0){

      print(elements[i])
    }

    else{

      print(names(input_list)[nums])
    }
  }

}

obtain_subsection_ids(section_name = "Model", "Model settings")

### Subsections

#### Overview ####
### Authorship ###
obtain_subsection_ids(section_name = "Overview", "Authorship")

obtain_description("o_authorship_1")
input_list$o_authorship_1 = "BioDT species distribution models"

obtain_description("o_authorship_3")
input_list$o_authorship_3 = "simrol@ceh.ac.uk; dylcar@ceh.ac.uk"

obtain_description("o_authorship_4")
input_list$o_authorship_4 = ""

### Model objective ###
obtain_subsection_ids(section_name = "Overview", "Model objective") 

obtain_description("o_objective_1")
input_list$o_objective_1 = "Mapping and interpolation"

obtain_description("o_objective_2")
input_list$o_objective_2 = "Maps of species presence"

### Focal Taxon ###
obtain_subsection_ids(section_name = "Overview", "Focal Taxon") 
obtain_description("o_taxon_1")
input_list$o_taxon_1 = target_species

### Location ###
obtain_subsection_ids(section_name = "Overview", "Location") 

obtain_description("o_location_1")
input_list$o_location_1 = "Cairngorms National Park, United Kingdom"

### scale of analysis ###
obtain_subsection_ids(section_name = "Overview", "Scale of Analysis")

# Get the extent coordinates
extent <- ext(output_raster_object)
# Additional field that records URL
input_list$location_url = "https://cairngorms.co.uk/"
# Obtain longitude and latitude coordinates bounding the tif file
obtain_description("o_scale_1_xmin")
input_list$o_scale_1_xmin = as.character(extent[1])
obtain_description("o_scale_1_xmax")
input_list$o_scale_1_xmax = as.character(extent[2])
obtain_description("o_scale_1_ymin")
input_list$o_scale_1_ymin = as.character(extent[3])
obtain_description("o_scale_1_ymax")
input_list$o_scale_1_ymax = as.character(extent[4])

obtain_description("o_scale_2")
input_list$o_scale_2 = "0.1 m"
obtain_description("o_scale_3")
input_list$o_scale_3 = "There is no temporal extent to the analysis"
obtain_description("o_scale_4")
input_list$o_scale_4 = "There is no temporal resolution to the analysis"
obtain_description("o_scale_5")
input_list$o_scale_5 = "political"

### Biodiversity data ###
obtain_subsection_ids(section_name = "Overview", "Biodiversity data")

obtain_description("o_bio_1")
input_list$o_bio_1 = "citizen science; field survey"
obtain_description("o_bio_2")
input_list$o_bio_2 = "point occurrence"

### Hypothesis ###
obtain_subsection_ids(section_name = "Overview", "Hypotheses") 

obtain_description("o_concept_1")
input_list$o_concept_1 = paste("investigating how environment variables affect the distributions of the species,", target_species, "in the Cairngorms National Park")

### Assumptions ###
obtain_subsection_ids(section_name = "Overview", "Assumptions")

obtain_description("o_assumptions_1")
input_list$o_assumptions_1 = "We assume that there is no temporal changes in environmental variables, that the abiotic variables is the sole predictor of species distributions other than biotic variables"

### Algorithms  ###
obtain_subsection_ids(section_name = "Overview", "Algorithms")

obtain_description("o_algorithms_1")
input_list$o_assumptions_1 = "glm; svm; gaussian process model"
obtain_description("o_algorithms_2")
input_list$o_assumptions_2 = "A variety of models were used without making run time too extensive"
obtain_description("o_algorithms_3")
input_list$o_assumptions_3 = "We calculated a mean weighted average based on model performance"

### Workflow ###
obtain_subsection_ids(section_name = "Overview", "Workflow") 

obtain_description("o_workflow_1")
input_list$o_workflow_1 = "Species occurence data from the Cairngorms, Scotland was obtained by download from GBIF. We filtered environmental variables to only include environment data from within a 5 km buffer of recorded occurences, and conducted spatial thinning. Using 4-fold partitioning, a series of spatial models were developed and validated. An ensemble model was created from the model series. We corrected for overprediction using posteriori methods"

### Software ###
obtain_subsection_ids(section_name = "Overview", "Software")

obtain_description("o_software_1")

# Get session information
sess_info <- sessionInfo()

# Extract information about loaded packages
loaded_packages <- sess_info$otherPkgs

# Concatenate package names with their versions
package_version_strings <- sapply(names(loaded_packages), function(pkg) {
  paste(pkg, "version", loaded_packages[[pkg]]$Version, sep = " ")
})

# Combine into a single long string, separated by commas and spaces
packages_versions <- paste(package_version_strings, collapse = " \n\n")

input_list$o_software_1 = paste("Written using", R.Version()$version.string, "with packages:\n\n", packages_versions)
obtain_description("o_software_2")
input_list$o_software_2 = "https://github.com/BioDT/uc-ces/tree/main/biodiversity_model"
obtain_description("o_software_3")
input_list$o_software_3 = paste("data obtained from GBIF API with DOI:", doi)

#### Data ####
### Biodiversity data ###
obtain_subsection_ids(section_name = "Data", "Biodiversity data") 

obtain_description("d_bio_1")
input_list$d_bio_1 = paste("Species: ", target_species,
      ", phylum: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(phylum) %>% unique(), ", order: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(order) %>% unique(),
      ", family: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(family) %>% unique(), sep = "")
obtain_description("d_bio_2")
input_list$d_bio_2 = "GBIF taxonomic backbone"
obtain_description("d_bio_3")
input_list$d_bio_3 = "species"
obtain_description("d_bio_4")
input_list$d_bio_4 = paste0("data obtained from GBIF API with DOI: ", doi, " at datetime: " access_datetime)
obtain_description("d_bio_5")
input_list$d_bio_5 = "opportunistic data"
obtain_description("d_bio_6")
input_list$d_bio_6 = paste("species: ", target_species, ", sample size = ", gbif_data %>% filter(scientificName == target_species) %>% nrow(), sep = "")
obtain_description("d_bio_7")
input_list$d_bio_7 = "No mask was used" # may be filled programatically
obtain_description("d_bio_8")
input_list$d_bio_8 = paste0("Spatial thinning: ", script_search("occfilt_env"), ifelse(script_search("occfilt_env"), "\n\nThinned occurrences based on environmental space", ""), "\n\n",
"temporal thinning: FALSE"
)
obtain_description("d_bio_9")
input_list$d_bio_9 = "no cleaning/filtering steps"
obtain_description("d_bio_10")
input_list$d_bio_10 = "not applicable"
obtain_description("d_bio_11")
input_list$d_bio_11 = paste0("Species occurences plotted for only species: ", target_species, "\n\n", "Spatial buffer: ", script_search("calib_area"), ifelse(script_search("calib_area"), "\n\nEstablished spatial buffers from occurences with 5 km radius", ""))
obtain_description("d_bio_12")
input_list$d_bio_12 = ""

### Data partitioning ###
obtain_subsection_ids(section_name = "Data", "Data partitioning") 

obtain_description("d_part_1")
input_list$d_part_1 = paste0("random partitioning: ", script_search("part_random"), ifelse(script_search("part_random"), paste("\n\n", "Conducted in flexsdm using 4 fold random partitioning"), ""))
obtain_description("d_part_2")
input_list$d_part_2 = "we calculate TSS, the threshold at which sensitivity and specificity are equal, as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model"
obtain_description("d_part_3")
input_list$d_part_3 = paste0("Random partitioning: ", script_search("part_random"), ifelse(script_search("part_random"), paste("\n\n", "Conducted in flexsdm using 4 fold random partitioning"), ""))

### Predictor variables ###
obtain_subsection_ids(section_name = "Data", "Predictor variables") 

obtain_description("d_pred_1")
input_list$d_pred_1 = paste(names(env_data), collapse = ", ")
obtain_description("d_pred_2")
input_list$d_pred_2 = "Google earth engine"

extent <- ext(env_data)
obtain_description("d_pred_3")
input_list$d_pred_3_xmin = extent[1]
input_list$d_pred_3_xmax = extent[2]
input_list$d_pred_3_ymin = extent[3]
input_list$d_pred_3_ymax = extent[4]

obtain_description("d_pred_4")
input_list$d_pred_4 = "0.1 km"
obtain_description("d_pred_5")
input_list$d_pred_5 = crs(env_data, describe = T)$name
obtain_description("d_pred_6")
input_list$d_pred_6 = "No temporal extent"
obtain_description("d_pred_7")
input_list$d_pred_7 = "No temporal resolution"
obtain_description("d_pred_8")
input_list$d_pred_8 = "no upscaling/downscaling"
obtain_description("d_pred_9")
input_list$d_pred_9 = "" # Do we know of any biases
obtain_description("d_pred_10")
input_list$d_pred_10 = "" # What is "dimension reduction of variables"?

### Transfer data ###
obtain_subsection_ids(section_name = "Data", "Transfer data") 

obtain_description("d_proj_1")
input_list$d_proj_1 = paste0("data obtained from GBIF API with DOI: ", doi, " at datetime: " access_datetime)
obtain_description("d_proj_2")
input_list$d_proj_2_xmin <- "Not applicable"
input_list$d_proj_2_xmax <- "Not applicable"
input_list$d_proj_2_ymin <- "Not applicable"
input_list$d_proj_2_ymax <- "Not applicable"
obtain_description("d_proj_3")
input_list$d_proj_3 = "Not applicable"
obtain_description("d_proj_4")
input_list$d_proj_4 = "Not applicable"
obtain_description("d_proj_5")
input_list$d_proj_5 = "Not applicable"
obtain_description("d_proj_6")
input_list$d_proj_6 = "Not applicable"
obtain_description("d_proj_7")
input_list$d_proj_7 = "Not applicable"
obtain_description("d_proj_8")
input_list$d_proj_8 = "Not applicable"

#### Model ####
### Variable pre-selection ###
obtain_subsection_ids(section_name = "Model", "Variable pre-selection")

obtain_description("m_preselect_1")
input_list$m_preselect_1 = "Not applicable"

### Multicollinearity ###
obtain_subsection_ids(section_name = "Model", "Multicollinearity")
obtain_description("m_multicol_1")
input_list$m_multicol_1 = "No methods used to handle collinearity"

### Model settings ###
obtain_subsection_ids(section_name = "Model", "Model settings")
obtain_description("m_settings_1")

# Create values for the m_settings_1 table
input_list$m_settings_1 <- data.frame(
  Model = c("Gaussian", "GLM", "SVM"),
  Family = c("gaussian", "gaussian", NA),
  Formula = paste("predictors:", paste(names(env_data), collapse = "; ")),
  Weights = "none",
  Notes = ""
)
obtain_description("m_settings_2")
input_list$m_settings_2 <- "Not applicable"

### Model estimates ###
obtain_subsection_ids(section_name = "Model", "Model estimates")

obtain_description("m_estim_1")
input_list$m_estim_1 <- "Not applicable"
obtain_description("m_estim_2")
input_list$m_estim_2 <- "No quantification"
obtain_description("m_estim_3")
input_list$m_estim_3 <- "No assessment"

### Model selection - model averaging - ensembles ###
obtain_subsection_ids(section_name = "Model", "Model selection - model averaging - ensembles")

obtain_description("m_selection_1")
input_list$m_selection_1 <- "We included all environment variables recorded in a model input raster spanning the Cairngorms, Scotland"
obtain_description("m_selection_2")
input_list$m_selection_2 <- "No variable weights were used"
obtain_description("m_selection_3")
input_list$m_selection_3 <- "Occurences obtained from the Global Biodiversity Information Facility (GBIF), with pseudo replication of absences. See model settings table for model classes and parameters"

### Analysis and Correction of non-independence ###
obtain_subsection_ids(section_name = "Model", "Analysis and Correction of non-independence")

obtain_description("m_depend_1")
input_list$m_depend_1 <- "No method"
obtain_description("m_depend_2")
input_list$m_depend_2 <- "No method"
obtain_description("m_depend_3")
input_list$m_depend_3 <- "No method"

### Threshold selection ###
obtain_subsection_ids(section_name = "Model", "Threshold selection")

obtain_description("m_threshold_1")
input_list$m_threshold_1 <- "Not applicable"

#### Assessment ####
### Performance statistics ###
obtain_subsection_ids(section_name = "Assessment", "Performance statistics")

obtain_description("a_perform_1")
input_list$a_perform_1 <- "Not applicable"
obtain_description("a_perform_2")
input_list$a_perform_2 <- "For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson)."
obtain_description("a_perform_3")
input_list$a_perform_3 <- "Not applicable."

### Plausibility check ###
obtain_subsection_ids(section_name = "Assessment", "Plausibility check")

obtain_description("a_plausibility_1")
input_list$a_plausibility_1 <- "No response plots"
obtain_description("a_plausibility_2")
input_list$a_plausibility_2 <- "No expert judgements"

#### Prediction ####
### Prediction output ###
obtain_subsection_ids(section_name = "Prediction", "Prediction output")

obtain_description("p_output_1")
input_list$p_output_1 <- "Species proportional occurence"
obtain_description("p_output_2")
input_list$p_output_2 <- paste0("Adjustments for overprediction: ", script_search("msdm_posteriori"), ifelse(script_search("msdm_posteriori"), "\n\nThe overprediction of SDMs was corrected for based on occurrence records and suitability patterns.", ""))

### Uncertainty quantification ###
obtain_subsection_ids(section_name = "Prediction", "Uncertainty quantification")

obtain_description("p_uncertainty_1")
input_list$p_uncertainty_1 <- "Not applicable"
obtain_description("p_uncertainty_2")
input_list$p_uncertainty_2 <- "The models are trained using GBIF datasets. There may be biases introduced by the method(s) of data collection and source contributor(s)"
obtain_description("p_uncertainty_3")
input_list$p_uncertainty_3 <- "Not applicable"
obtain_description("p_uncertainty_4")
input_list$p_uncertainty_4 <- ""
obtain_description("p_uncertainty_5")
input_list$p_uncertainty_5 <- "No visualisation or treatment"
```

### Render ODMAP report

Here, we generate a final report for ODMAP as a word document to be included as supplementary information. 
```{r}
odmap_dict <- read_csv("odmap_dict.csv")
completed_fields <- read.csv("C:/Users/dylcar/OneDrive - UKCEH/Desktop/BioDT/uc-ces/metadata/biodiversity_model_od_map_fields.csv")
odmap_dict <- left_join(odmap_dict, completed_fields)

authors_string = paste(input_list$first_name, input_list$last_name, collapse = ", ")
  # Render the R Markdown to a Word document
render(
  input = file.path(getwd(), "ODMAP_generate.Rmd"),
  output_format = "word_document",
  output_file = file.path(getwd(), "sdm_odmap.docx"))

shell.exec("sdm_odmap.docx")

```