---
title: "BioDT biodiversity model"
date: "`r Sys.Date()`"
output: html_document
params:
  taxonkey: 5334220
  out_file: "untitled.tif"
  n_bootraps: 5
---


```{r, include=FALSE}
# testing 
params <- list(taxonkey= 5334220, out_file= "untitled.tif", n_bootraps= 6)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages

```{r packages, warning=FALSE}
#spatial
library(terra)
library(sf)

#statistical
library(flexsdm)

#getting gbif data
library(rgbif)

#spatial thinning
library(spThin)

#data manipulation
library(dplyr)

# Load markdown for ODMAP render
library(rmarkdown)

# For reading ODMAP template
library(readr)

set.seed(42)

```

## Load data

### Species data from GBIF

```{r load_gbif_data}
#get the GBIF data from the API
gbif_data <- occ_data(taxonKey = params$taxonkey,
         gadmGid = "GBR.3_1", # scotland
         limit=10000,
         coordinateUncertaintyInMeters='0,101',
         occurrenceStatus = NULL,
         hasCoordinate=TRUE,
         hasGeospatialIssue=FALSE)

#get the appropriate columns
gbif_data <- gbif_data$data %>% select(key, 
                                       scientificName, 
                                       phylum,
                                       order,
                                       family,
                                       y = decimalLatitude,
                                       x = decimalLongitude,
                                       occurrenceStatus,
                                       datasetKey,
                                       publishingOrgKey,
                                       coordinateUncertaintyInMeters,
                                       year,
                                       month,
                                       day,
                                       license,
                                       recordedBy,
                                       identifiedBy,
                                       rightsHolder
                                       )
#set presence absence
gbif_data$pr_ab <- 0
gbif_data$pr_ab[gbif_data$occurrenceStatus == "PRESENT"] <- 1


gbif_xy <- gbif_data %>% select(x,
                                y)

#Species name
target_species = gbif_data$scientificName %>% first()
print(target_species)
```

### Environmental

```{r load_environmental_data}
#Currently loading a raster file locally but this could be upgraded to any other raster
env_data <- rast("inputs/env-layers.tif")

env_data_low_res <- aggregate(env_data, fact = 10)

```

### FlexSDM workflow

#### Delimit of a calibration area

Delimiting the calibration area (aka accessible area) is an essential step in SDMs both in methodological and theoretical terms. The calibration area will affect several characteristics of a SDM like the range of environmental variables, the number of absences, the distribution of background points and pseudo-absences, and unfortunately, some performance metrics like AUC and TSS. There are several ways to delimit a calibration area. In calib_area(). We used a method that the calibration area is delimited by a 5-km buffer around presences (shown in the figure below).

```{r calibation_area}
ca <-
  calib_area(
    data = gbif_data,
    x = 'x',
    y = 'y',
    method =  c('buffer', width = 5000),
    crs = crs(env_data)
  ) # create a calibration area with 100 km buffer around occurrence points


# visualize the species occurrences
layer1 <- env_data_low_res[[8]]

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")

```

#### Occurrence filtering

Sample bias in species occurrence data has long been a recognized issue in SDM. However, environmental filtering of observation data can improve model predictions by reducing redundancy in environmental (e.g. climatic) hyper-space (Varela et al. 2014). Here we will use the function occfilt_env() to thin the red fir occurrences based on environmental space. This function is unique to flexsdm, and in contrast with other packages is able to use any number of environmental dimensions and does not perform a PCA before filtering.

Next we apply environmental occurrence filtering using 8 bins and display the resulting filtered occurrence data

```{r occ_filtering}
gbif_data$id <- 1:nrow(gbif_data) # adding unique id to each row
gbif_data_f <- gbif_data %>%
  occfilt_env(
    data = .,
    x = "x",
    y = "y",
    id = "id",
    nbins = 8,
    env_layer = env_data
  ) %>%
  left_join(gbif_data, by = c("id", "x", "y"))

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data[,c("x", "y")], col = "#00000480")
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
```

#### Random partition with 4 folds

Data partitioning, or splitting data into testing and training groups, is a key step in building SDMs. flexsdm offers multiple options for data partitioning and here we use a spatial block method. Geographically structured data partitioning methods are especially useful if users want to evaluate model transferability to different regions or time periods. The part_sblock() function explores spatial blocks with different raster cells sizes and returns the one that is best suited for the input datset based on spatial autocorrelation, environmental similarity, and the number of presence/absence records in each block partition. The function’s output provides users with 1) a tibble with presence/absence locations and the assigned partition number, 2) a tibble with information about the best partition, and 3) a SpatRaster showing the selected grid. Here we want to divide the data into 4 different partitions using the spatial block method.

```{r partitioning}
set.seed(10)
gbif_data_f_partitioned <- gbif_data_f %>%
  part_random(
    data = .,
    pr_ab = "pr_ab",
    method = c(method = "kfold", folds = 4)
  )

# n points per block type
gbif_data_f_partitioned %>%
  dplyr::group_by(.part) %>%
  dplyr::count()
```

#### Sample background

The function sample_background() allows selection of background sample points based on different geographic restrictions and sampling methods. Here, we sample a set of background points based on our earlier spatial block partitioning using the “random” method. Using lapply() in this case ensures that we generate background points in each of our spatial blocks (n = 2). We are also specifying that we want ten times the amount of background points as our original occurrences and that our calibration area will be the buffer area around presence points (see section on “Calibration area”).

```{r background_samples}
set.seed(10)
bg <- gbif_data_f_partitioned %>% 
  split(f=gbif_data_f_partitioned$.part) %>%
  lapply(function(data){
    sample_background(
      data = data,
      x = "x",
      y = "y",
      n = nrow(data),
      # number of background points to be sampled
      method = "random",
      rlayer = env_data,
      calibarea = ca # A SpatVector which delimit the calibration area used for a given species
    ) %>%
      mutate(.part = data$.part[1])
      
  }) %>% 
  bind_rows()
  
gbif_data_pa <- bind_rows(gbif_data_f_partitioned, bg)

plot(layer1)
plot(crop(ca, layer1), add=TRUE)
points(gbif_data_f[,c("x", "y")], col = "#5DC86180")
points(bg[,c("x", "y")], col = "red")

#extract data from raster
gbif_data_pa <- gbif_data_pa %>% sdm_extract(
    data = .,
    x = "x",
    y = "y",
    env_layer = env_data,
    filter_na = TRUE
  )

gbif_data_pa %>% select(names(env_data))%>% is.na() %>%colSums()
```

#### Model fitting

Now, fit our models. The flexsdm package offers a wide range of modeling options, from traditional statistical methods like GLMs and GAMs, to machine learning methods like random forests and support vector machines. For each modeling method, flexsdm provides both fit_ and tune_ functions, which allow users to use default settings or adjust hyperparameters depending on their research goals. Here, we will test out tune_max() (tuned Maximum Entropy model), fit_gau() (fit Guassian Process model), and fit_glm (fit Generalized Linear Model). For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson). In this example, we selected TSS as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model.

```{r model fitting}
f_gau <- fit_gau(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen")
)
f_gau$performance %>% kable()

f_glm <- fit_glm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  poly = 2
)
f_glm$performance %>% kable()

f_svm <- fit_svm(
  data = gbif_data_pa,
  response = "pr_ab",
  predictors = names(env_data),
  partition = ".part",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
)
f_svm$performance %>% kable()

```


#### Model veriabitiy

Calculate variability 

```{r model_variability}
#variability between model types
# individ_models <- sdm_predict(
#   models = list(f_gau,f_glm,f_svm),
#   pred = env_data,
#   thr = NULL,
#   con_thr = FALSE,
#   predict_area = NULL
# )
# 
# variability <- rast(individ_models) %>% diff() %>% abs() 
# plot(variability,col=cl)


```

#### Build ensemble model

Spatial predictions from different SDM algorithms can vary substantially, and ensemble modeling has become increasingly popular. With the fit_ensemble() function, users can easily produce an ensemble SDM based on any of the individual fit_ and tune_ models included the package. In this example, we fit an ensemble model for red fir based on the weighted average of the three individual models. We used the same threshold values and performance metric that were implemented in the individual models.

```{r build_ensemble}
ens_m <- fit_ensemble(
  models = list(f_gau, f_glm, f_svm),
  ens_method = "meanw",
  thr = c("max_sens_spec", "equal_sens_spec", "max_sorensen"),
  thr_model = "max_sens_spec",
  metric = "TSS"
)

model_perf <- sdm_summarize(list(f_gau, f_glm, f_svm, ens_m))
knitr::kable(model_perf)

```

#### Project the ensemble model

Next we project the ensemble model in space across the entire extent of our environmental layer, using the sdm_predict() function. This function can be use to predict species suitability across any area for species’ current or future suitability. In this example, we only project the ensemble model with one threshold, though users have the option to project multiple models with multiple threshold values. Here, we also specify that we want the function to return a SpatRast with continuous suitability values above the threshold (con_thr = TRUE).

```{r project_model}
pr_1 <- sdm_predict(
  models = ens_m,
  pred = env_data,
  thr = "max_sens_spec",
  con_thr = TRUE,
  predict_area = NULL
)
#> Predicting ensembles

unconstrained <- pr_1$meanw[[1]]
names(unconstrained) <- "unconstrained"

cl <- rev(c("#FDE725", "#B3DC2B", "#6DCC57", "#36B677", "#1F9D87", "#25818E", "#30678D", "#3D4988", "#462777", "#440154"))
plot(unconstrained, col=cl)
```

#### Constrain the model with msdm_posterior

Finally, flexsdm offers users function that help correct overprediction of SDM based on occurrence records and suitability patterns. In this example we constrained the ensemble model using the method “occurrence based restriction”, which assumes that suitable patches that intercept species occurrences are more likely a part of species distributions than suitable patches that do not intercept any occurrences. Because all methods of the msdm_posteriori() function work with presences it is important to always use the original database (i.e., presences that have not been spatially or environmentally filtered). All of the methods available in the msdm_posteriori() function are based on Mendes et al. (2020).

```{r constrain_model}
thr_val <- ens_m$performance %>%
  dplyr::filter(threshold == "max_sens_spec") %>%
  pull(thr_value)

m_pres <- msdm_posteriori(
  records = gbif_data,
  x = "x",
  y = "y",
  pr_ab = "pr_ab",
  cont_suit = pr_1$meanw[[1]],
  method = c("obr"),
  thr = c("sensitivity", sens = thr_val),
  buffer = NULL
)

constrained <- m_pres$meanw[[1]]
names(constrained) <- "constrained"
plot(constrained, col=cl)

```


#### Distance from nearest point

```{r distance_from_record}
# create a blank vector with band name 'distance' and values are NA
distance_rast <- env_data[[1]]
values(distance_rast) <-NA
names(distance_rast) <- "distance"

#set grid cells with values to non-NA value (-1)
values(distance_rast)[cellFromXY(distance_rast,xy=as.matrix(gbif_data[,c("x","y")]))] <- -1

#create a raster of distance from nearest record
distance_rast <- distance(distance_rast)/1000 #distance from cells in km
distance_rast_scaled <- min(distance_rast,5)/5 #cap a 5 km and scale to 0-1
plot(distance_rast_scaled)


# create a raster of suitable, but far from record
suitable_unrecorded <- unconstrained*distance_rast_scaled
plot(suitable_unrecorded, col=cl)
```

## Export

Export the model the output file location provided to the parametrised markdown

```{r export}
model_outputs <- constrained
model_outputs$suitable_unrecorded <-suitable_unrecorded
plot(model_outputs, col=cl)

#save raster
model_outputs %>% writeRaster(params$out_file,overwrite=T)

#save model
list(f_gau, f_glm, f_svm, ens_m) %>% saveRDS(gsub(".tif",".RDS",params$out_file))
```


## Generate ODMAP inputs

Here, we use ODMAP (Overview, Data, Model, Assessment and Prediction) to document our SDM model output and workflow
```{r, message = FALSE, warning = FALSE}
input_list = readRDS(file.path(getwd(), "ODMAP_template_list.RDS"))

# Generate template dictionary
odmap_dict <- read_csv("odmap_dict.csv")
completed_fields <- read.csv("C:/Users/dylcar/OneDrive - UKCEH/Desktop/BioDT/uc-ces/metadata/biodiversity_model_od_map_fields.csv")
odmap_dict <- left_join(odmap_dict, completed_fields)

# Load your SpatRaster object (replace 'raster_object' with your actual object)
output_raster_object <- rast("untitled.tif")

# Define a function to search the script code use
script_search <- function(regex) {
lines <- readLines("biodiversity_model_workflow_flexsdm_ODMAP.Rmd")
any(sapply(lines, FUN = function(line){
  trimmed_line <- trimws(line) # Remove leading and trailing whitespace
  searches = !startsWith(trimmed_line, "#") && grepl(regex, trimmed_line) && !grepl("script_search", trimmed_line)
}))
}

# Function to obtain a description for a given ID from a dictionary data frame
obtain_description <- function(id) {
  # Check if the ID exists in the dictionary
  if (!(id %in% odmap_dict$element_id)) {
    last_underscore <- max(gregexpr("_", id)[[1]])
    id <- substr(id, 1, last_underscore - 1)
  }

  # Retrieve description and text entry for the ID
  description = odmap_dict %>% filter(element_id == id) %>% pull(element_placeholder)
  text_entry = odmap_dict %>% filter(element_id == id) %>% pull(Value)

  # Print the description and text entry if available
  if (!is.na(text_entry) && identical(text_entry, character(0)) == FALSE && text_entry != "") {
    print(paste0(description, "    Entry found: ", text_entry))
  } else {
    print(description)
  }
}

# Function to obtain elements in each subsection from a dictionary data frame
obtain_subsection_ids <- function(section_name, subsection_text) {
  # Filter the dictionary for the given subsection
  odmap_dict_sub = odmap_dict %>% filter(section == section_name, subsection == subsection_text)
  elements = odmap_dict_sub$element_id

  # Process names in the input list
  input_list_stemmed_names = unlist(sapply(names(input_list), USE.NAMES = F, FUN = function(name) {
    last_underscore <- max(gregexpr("_", name)[[1]])
    substr(name, 1, last_underscore - 1)
  }))

  # Print the names of elements in the subsection
  for (i in seq_along(elements)) {

    nums = which(input_list_stemmed_names == elements[i])

    if (length(nums) == 0){

      print(elements[i])
    }

    else{

      print(names(input_list)[nums])
    }
  }

}

obtain_subsection_ids(section_name = "Model", "Model settings")

### Subsections

#### Overview ####
### Authorship ###
obtain_subsection_ids(section_name = "Overview", "Authorship")

obtain_description("o_authorship_1")
input_list$o_authorship_1 = "BioDT species distribution models"

obtain_description("o_authorship_3")
input_list$o_authorship_3 = "simrol@ceh.ac.uk; dylcar@ceh.ac.uk"

obtain_description("o_authorship_4")
input_list$o_authorship_4 = ""

### note there is no author names stored inputs

### Model objective ###
obtain_subsection_ids(section_name = "Overview", "Model objective") 

obtain_description("o_objective_1")
input_list$o_objective_1 = "Mapping and interpolation"

obtain_description("o_objective_2")
input_list$o_objective_2 = "Maps of species presence"

### Focal Taxon
obtain_subsection_ids(section_name = "Overview", "Focal Taxon") 
obtain_description("o_taxon_1")
input_list$o_taxon_1 = target_species

### Location subsection ###
obtain_subsection_ids(section_name = "Overview", "Location") 

obtain_description("o_location_1")
input_list$o_location_1 = "Cairngorms National Park, United Kingdom"

### scale of analysis ###
obtain_subsection_ids(section_name = "Overview", "Scale of Analysis")

# Get the extent coordinates
extent <- ext(output_raster_object)

# Obtain longitude and latitude coordinates bounding the tif file

# Additional field that records URL
input_list$location_url = "https://cairngorms.co.uk/"

obtain_description("o_scale_1_xmin")
input_list$o_scale_1_xmin = as.character(extent[1])
obtain_description("o_scale_1_xmax")
input_list$o_scale_1_xmax = as.character(extent[2])
obtain_description("o_scale_1_ymin")
input_list$o_scale_1_ymin = as.character(extent[3])
obtain_description("o_scale_1_ymax")
input_list$o_scale_1_ymax = as.character(extent[4])

obtain_description("o_scale_2")
input_list$o_scale_2 = "0.1 m"
obtain_description("o_scale_3")
input_list$o_scale_3 = "There is no temporal extent to the analysis"
obtain_description("o_scale_4")
input_list$o_scale_4 = "There is no temporal resolution to the analysis"
obtain_description("o_scale_5")
input_list$o_scale_5 = "political"

### Biodiversity data subsection ###
obtain_subsection_ids(section_name = "Overview", "Biodiversity data")

obtain_description("o_bio_1")
input_list$o_bio_1 = "citizen science; field survey"
obtain_description("o_bio_2")
input_list$o_bio_2 = "point occurrence"

### Hypothesis ###
obtain_subsection_ids(section_name = "Overview", "Hypotheses") 

obtain_description("o_concept_1")
input_list$o_concept_1 = paste("investigating how environment variables affect the distributions of the species,", target_species, "in the Cairngorms National Park")

### Assumptions ###
obtain_subsection_ids(section_name = "Overview", "Assumptions")

obtain_description("o_assumptions_1")
input_list$o_assumptions_1 = "We assume that there is no temporal changes in environmental variables, that the abiotic variables is the sole predictor of species distributions other than biotic variables."

### Algorithms  ###
obtain_subsection_ids(section_name = "Overview", "Algorithms")

obtain_description("o_algorithms_1")
input_list$o_assumptions_1 = "glm; svm; gaussian process model"
obtain_description("o_algorithms_2")
input_list$o_assumptions_2 = " Avariety of models were used without making run time too extensive"
obtain_description("o_algorithms_3")
input_list$o_assumptions_3 = "We calculated a mean weighted average based on model performance"

### Workflow ###
obtain_subsection_ids(section_name = "Overview", "Workflow") 

obtain_description("o_workflow_1")
input_list$o_workflow_1 = "Pulled data from GBIF, conducts data thinning, fits model, and builds ensemble from models and predicts in space."

### Software ###
obtain_subsection_ids(section_name = "Overview", "Software")

obtain_description("o_software_1")

# Get session information
sess_info <- sessionInfo()

# Extract information about loaded packages
loaded_packages <- sess_info$otherPkgs

# Concatenate package names with their versions
package_version_strings <- sapply(names(loaded_packages), function(pkg) {
  paste(pkg, "version", loaded_packages[[pkg]]$Version, sep = " ")
})

# Combine into a single long string, separated by commas and spaces
packages_versions <- paste(package_version_strings, collapse = " \n\n")

input_list$o_software_1 = paste("Written using", R.Version()$version.string, "with packages,", packages_versions)
obtain_description("o_software_2")
input_list$o_software_2 = "https://github.com/BioDT/uc-ces/tree/main/biodiversity_model"
obtain_description("o_software_3")
input_list$o_software_3 = paste("data obtained from GBIF API with DOI:", gbif_data %>% pull(datasetKey) %>% unique(), collapse = " ")

#### Data ####
obtain_subsection_ids(section_name = "Data", "Biodiversity data") 

obtain_description("d_bio_1")
input_list$d_bio_1 = paste("species: ", target_species,
      ", phylum: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(phylum) %>% unique(), ", order: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(order) %>% unique(),
      ", family: ", gbif_data %>% filter(scientificName == target_species) %>% first() %>% pull(family) %>% unique(), sep = "")
obtain_description("d_bio_2")
input_list$d_bio_2 = "GBIF taxonomic backbone"
obtain_description("d_bio_3")
input_list$d_bio_3 = "species"
obtain_description("d_bio_4")

input_list$d_bio_4 = paste("data obtained from GBIF API with DOI:", gbif_data %>% pull(datasetKey) %>% unique(),
                           "accessed at:", 
                           # Get current date and time, and save in string format
                            format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                           collapse = " \n\n")

obtain_description("d_bio_5")
input_list$d_bio_5 = "opportunistic data"
obtain_description("d_bio_6")
input_list$d_bio_6 = paste("species: ", target_species, ", sample size = ", gbif_data %>% filter(scientificName == target_species) %>% nrow(), sep = "")
obtain_description("d_bio_7")
input_list$d_bio_7 = "No mask was used" # may be filled programatically
obtain_description("d_bio_8")
input_list$d_bio_8 = paste("spatial thinning:", script_search("occfilt_env"), ifelse(script_search("occfilt_env"), "Thins occurrences based on environmental space, with function occfilt_env()", ""), "\n\n",
"temporal thinning: FALSE" #script_search("")
)

obtain_description("d_bio_9")
input_list$d_bio_9 = paste("no cleaning/filtering steps") # beyond spatial thinning, no transformations are used
obtain_description("d_bio_10")
input_list$d_bio_10 = "not applicable"
obtain_description("d_bio_11")
input_list$d_bio_11 = paste("Species occurences plotted for only species:", target_species, "\n\n", "Spatial buffer:", script_search("calib_area"), ifelse(script_search("calib_area"), "spatial buffer established with function calib_area()", ""))
obtain_description("d_bio_12")
input_list$d_bio_12 = "" # Needs manual input

obtain_subsection_ids(section_name = "Data", "Data partitioning") 

obtain_description("d_part_1")
input_list$d_part_1 = paste("random_partitioning:", script_search("part_random"), ifelse(script_search("part_random"), paste("\n\n", "conducted automatically in flexsdm using 4 fold random partitioning"), ""))
obtain_description("d_part_2")
input_list$d_part_2 = "we calculate TSS, the threshold at which sensitivity and specificity are equal, as the performance metric used for selecting the best combination of hyper-parameter values in the tuned Maximum Entropy model."
# Randomly selected 4 folds using k-fold partitioning

obtain_description("d_part_3")
input_list$d_part_3 = paste("random_partitioning:", script_search("part_random"), ifelse(script_search("part_random"), paste("\n\n", "conducted automatically in flexsdm using 4 fold random partitioning"), ""))

obtain_subsection_ids(section_name = "Data", "Predictor variables") 

obtain_description("d_pred_1")
input_list$d_pred_1 = paste(names(env_data), collapse = ", ")
obtain_description("d_pred_2")
input_list$d_pred_2 = "Google earth engine"

# Extract xmin, xmax, ymin, and ymax of input raster
## What is the difference between the spatial extent of the 'scale of analysis' and spatial extent of the Data? Are they the same here?
extent <- ext(env_data)

obtain_description("d_pred_3")
input_list$d_pred_3_xmin = extent[1]
input_list$d_pred_3_xmax = extent[2]
input_list$d_pred_3_ymin = extent[3]
input_list$d_pred_3_ymax = extent[4]

obtain_description("d_pred_4")
input_list$d_pred_4 = "0.1 km"
obtain_description("d_pred_5")
input_list$d_pred_5 = crs(env_data, describe = T)$name
obtain_description("d_pred_6")
input_list$d_pred_6 = "No temporal extent"
obtain_description("d_pred_7")
input_list$d_pred_7 = "No temporal resolution"
obtain_description("d_pred_8")
input_list$d_pred_8 = "no upscaling/downscaling"
obtain_description("d_pred_9")
input_list$d_pred_9 = "" # Do we know of any biases
obtain_description("d_pred_10")
input_list$d_pred_10 = "" # What is "dimension reduction of variables"?

obtain_subsection_ids(section_name = "Data", "Transfer data") 
obtain_description("d_proj_1")
input_list$d_proj_1 = paste("random_partitioning:", script_search("part_random"), ifelse(script_search("part_random"), paste("\n\n", "Model was validated using only 4 fold random partitioning"), ""))
obtain_description("d_proj_2")
# Extract xmin, xmax, ymin, and ymax
input_list$d_proj_2_xmin <- "Not applicable"
input_list$d_proj_2_xmax <- "Not applicable"
input_list$d_proj_2_ymin <- "Not applicable"
input_list$d_proj_2_ymax <- "Not applicable"
obtain_description("d_proj_3")
input_list$d_proj_3 = "Not applicable"
obtain_description("d_proj_4")
input_list$d_proj_4 = "Not applicable"
obtain_description("d_proj_5")
input_list$d_proj_5 = "Not applicable"
obtain_description("d_proj_6")
input_list$d_proj_6 = "Not applicable"
obtain_description("d_proj_7")
input_list$d_proj_7 = "Not applicable"
obtain_description("d_proj_8")
input_list$d_proj_8 = "Not applicable"

#### Model ####
obtain_subsection_ids(section_name = "Model", "Variable pre-selection")

obtain_description("m_preselect_1")
input_list$m_preselect_1 = "Not applicable"

obtain_subsection_ids(section_name = "Model", "Multicollinearity")

obtain_description("m_multicol_1")
input_list$m_multicol_1 = "No methods used to handle collinearity"

obtain_subsection_ids(section_name = "Model", "Model settings")

obtain_description("m_settings_1")
# Create values for the m_settings_1 table
input_list$m_settings_1 <- data.frame(
  Model = c("Gaussian", "GLM", "SVM"),
  Family = c("gaussian", "gaussian", NA),
  Formula = paste("predictors:", paste(names(env_data), collapse = "; ")),
  Weights = "none",
  Notes = ""
)

obtain_description("m_settings_2")
input_list$m_settings_2 <- "Not applicable"

obtain_subsection_ids(section_name = "Model", "Model estimates")

obtain_description("m_estim_1")
input_list$m_estim_1 <- "Not applicable"

obtain_description("m_estim_2")
input_list$m_estim_2 <- "No quantification" # need to verify this with Simon

obtain_description("m_estim_3")
input_list$m_estim_3 <- "No assessment"

obtain_subsection_ids(section_name = "Model", "Model selection - model averaging - ensembles")

obtain_description("m_selection_1")
input_list$m_selection_1 <- "We included all environment variables recorded in a raster spanning the Cairngorms, Scotland."

obtain_description("m_selection_2")
input_list$m_selection_2 <- "No variable weights were used."

obtain_description("m_selection_3")
input_list$m_selection_3 <- "Occurences obtained from the Global Biodiversity Information Facility (GBIF), with pseudo replication of absences. See model settings table for model classes and parameters."

obtain_subsection_ids(section_name = "Model", "Analysis and Correction of non-independence")

obtain_description("m_depend_1")
input_list$m_depend_1 <- "No method."
obtain_description("m_depend_2")
input_list$m_depend_2 <- "No method."
obtain_description("m_depend_3")
input_list$m_depend_3 <- "No method."

obtain_subsection_ids(section_name = "Model", "Threshold selection")

obtain_description("m_threshold_1")
input_list$m_threshold_1 <- "For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson)."

#### Assessment ####
obtain_subsection_ids(section_name = "Assessment", "Performance statistics")

obtain_description("a_perform_1")
input_list$a_perform_1 <- "For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson)."
obtain_description("a_perform_2")
input_list$a_perform_2 <- "For each model, we selected three threshold values to generate binary suitability predictions: the threshold that maximizes TSS (max_sens_spec), the threshold at which sensitivity and specificity are equal (equal_sens_spec), and the threshold at which the Sorenson index is highest (max_sorenson)."
obtain_description("a_perform_3")
input_list$a_perform_3 <- "Not applicable."

obtain_subsection_ids(section_name = "Assessment", "Plausibility check")

obtain_description("a_plausibility_1")
input_list$a_perform_1 <- "No response plots"
obtain_description("a_plausibility_2")
input_list$a_perform_2 <- ""

#### Prediction ####
obtain_subsection_ids(section_name = "Prediction", "Prediction output")

obtain_description("p_output_1")
input_list$p_output_1 <- "species proportional occurence."
obtain_description("p_output_2")
input_list$p_output_2 <- ifelse(script_search("msdm_posteriori"), "Overprediction of SDMs corrected for based on occurrence records and suitability patterns.", "")

obtain_subsection_ids(section_name = "Prediction", "Uncertainty quantification")

obtain_description("p_uncertainty_1")
input_list$p_uncertainty_1 <- "Not applicable." # Check this with Simon
obtain_description("p_uncertainty_2")
input_list$p_uncertainty_2 <- "Not applicable." # Check this with Simon
obtain_description("p_uncertainty_3")
input_list$p_uncertainty_3 <- "Not applicable."
obtain_description("p_uncertainty_4")
input_list$p_uncertainty_4 <- ""
obtain_description("p_uncertainty_5")
input_list$p_uncertainty_5 <- "No visualisation."
```

### Render ODMAP report

Here, we generate a final report for ODMAP as a word document to be included as supplementary information. 
```{r}
odmap_dict <- read_csv("odmap_dict.csv")
completed_fields <- read.csv("C:/Users/dylcar/OneDrive - UKCEH/Desktop/BioDT/uc-ces/metadata/biodiversity_model_od_map_fields.csv")
odmap_dict <- left_join(odmap_dict, completed_fields)

authors_string = paste(input_list$first_name, input_list$last_name, collapse = ", ")
  # Render the R Markdown to a Word document
render(
  input = file.path(getwd(), "ODMAP_generate.Rmd"),
  output_format = "word_document",
  output_file = file.path(getwd(), "sdm_odmap.docx"))

shell.exec("sdm_odmap.docx")

```